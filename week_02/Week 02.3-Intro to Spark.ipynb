{"cells":[{"cell_type":"markdown","source":["# Intro to Apache Spark\n\n* [Intro to Spark slides](https://github.com/databricks/tech-talks/blob/master/2020-04-29%20%7C%20Intro%20to%20Apache%20Spark/Intro%20to%20Spark.pdf)\n* What is a Spark DataFrame?\n  * Read in the [NYT data set](https://github.com/nytimes/covid-19-data) \n* How to perform a distributed count?\n* Transformations vs. Actions\n* Spark SQL\n\n[Databrick File Systems DBFS](https://docs.databricks.com/data/databricks-file-system.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8807a2e5-0cb7-42d4-a74f-c377bb544d77"}}},{"cell_type":"code","source":["%fs ls databricks-datasets/COVID/covid-19-data/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f9f1e35-3dc2-4099-ba3e-54e788c9b55c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## How do we represent this data?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e2d7220-08b9-4f5f-b9e7-18db393e8d1d"}}},{"cell_type":"markdown","source":["![Unified Engine](https://files.training.databricks.com/images/105/unified-engine.png)\n\n\n####At first there were RDDs...\n* **R**esilient: Fault-tolerant\n* **D**istributed: Across multiple nodes\n* **D**ataset: Collection of partitioned data\n\nRDDs are immutable once created and keep track of their lineage to enable failure recovery.\n\n####... and then there were DataFrames\n* Higher-level APIs\n* User friendly\n* Optimizations and performance improvements\n\n![RDD vs DataFrames](https://files.training.databricks.com/images/105/rdd-vs-dataframes.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"263b2f12-2211-4f8f-a75f-f3b372dd593a"}}},{"cell_type":"markdown","source":["###Create a DataFrame from the NYT COVID data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae1ea4e3-6cb9-42d9-852d-7724f9bde510"}}},{"cell_type":"code","source":["covid_df = spark.read.csv(\"dbfs:/databricks-datasets/COVID/covid-19-data/us-counties.csv\")\ncovid_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7f9c945-a92b-4f9c-af6d-ab4209f62dea"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's look at the [Spark docs](https://spark.apache.org/docs/latest/index.html) to see what options we have to pass into the csv reader."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cae4c0f3-9072-4ae2-a416-4bbd1f9be3e6"}}},{"cell_type":"code","source":["covid_df = spark.read.csv(\"dbfs:/databricks-datasets/COVID/covid-19-data/us-counties.csv\", header=True, inferSchema=True)\ncovid_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e8de74f-ad97-40d9-bbd7-c18d13d6c2ce"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###How many records do we have?\n* Instead of counting M&Ms, let's count the number of rows in the DataFrame\n\n###What do we expect our Spark job to look like?\n* How many stages?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4cfba09-0cb9-47ad-9fe1-92939fd08cd4"}}},{"cell_type":"code","source":["covid_df.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9e53c8c-e9df-4787-832a-238b0889547b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Let's write some Spark code!\n\n* I want to look at only the information for the county I live in (Boston)\n* I want the most recent information at the top"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef4a3aff-92a8-47f1-a71b-820493fa161c"}}},{"cell_type":"code","source":["(covid_df\n .sort(covid_df[\"date\"].desc()) \n .filter(covid_df[\"county\"] == \"Providence\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3a991e6-e5a7-4572-b8cd-e2f9367a94d4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**...nothing happened. Why?**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3fca7e41-edb7-4667-8e1b-1443b2d4c23e"}}},{"cell_type":"markdown","source":["## Transformations vs Actions\n\nThere are two types of operations in Spark: transformations and actions.\n\nFundamental to Apache Spark are the notions that\n* Transformations are **LAZY**\n* Actions are **EAGER**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4a8c5de-cdf9-4595-b21a-096bfa8d4612"}}},{"cell_type":"markdown","source":["Why isn't is showing me results? **Sort** and **filter** are `transformations`, which are lazily evaluated in Spark.\n\nLaziness has a number of benefits\n* Not forced to load all data in the first step\n  * Technically impossible with **REALLY** large datasets.\n* Easier to parallelize operations \n  * N different transformations can be processed on a single data element, on a single thread, on a single machine. \n* Most importantly, it allows the framework to automatically apply various optimizations\n  * This is also why we use Dataframes!\n  \nThere's a lot Spark's **Catalyst** optimizer can do. Let's focus on only this situation. For more information, read [this blog!](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)\n  \n![Catalyst](https://files.training.databricks.com/images/105/catalyst-diagram.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc452ffc-7183-4da4-a782-00617e6728f5"}}},{"cell_type":"code","source":["(covid_df\n .sort(covid_df[\"date\"].desc()) \n .filter(covid_df[\"county\"] == \"Providence\") \n .show())  #action!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f05d5c8f-a78b-4910-b812-685412673331"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###We can see the optimizations in action!\n* Go to the Spark UI\n* Click on the SQL query associated with your Spark job\n* See the logical and physical plans!\n  * The filter and sort have been swapped"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0390ab67-7e66-43be-8881-9f454f327baa"}}},{"cell_type":"markdown","source":["## Spark SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b13cf1a4-eec4-4fd9-b4f8-79b318dc6ca2"}}},{"cell_type":"code","source":["covid_df.createOrReplaceTempView(\"covid\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0a6b23c-4578-4373-a20b-5401d67babe9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n\nSELECT * \nFROM covid\n\n-- keys = date, grouping = county, values = cases"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc96efa4-0c6d-4ee0-b2b0-7e2a0724ce0c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n\nSELECT * \nFROM covid \nWHERE county = \"Providence\"\n\n-- keys = date, grouping = county, values = cases, deaths"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69b6a3e8-c331-4fda-ae74-db8c1b45a3f7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n\nSELECT max(cases) AS max_cases, max(deaths) AS max_deaths, county \nFROM covid \nGROUP BY county \nORDER BY max_cases DESC\nLIMIT 10"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60002cc3-cd4b-4ded-a61a-2095286642c2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###Try your own analysis!\n* Here's an idea to get you started\n* There's a lot more examples [here](https://databricks.com/blog/2020/04/14/covid-19-datasets-now-available-on-databricks.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f89f2675-ee13-44e5-bd71-2f004944dbb7"}}},{"cell_type":"markdown","source":["We want to see whether the covid-19 cases/deaths is associated with population."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de47249d-865c-4902-a372-9ee7ebedc5d0"}}},{"cell_type":"markdown","source":["**This is census data taken from census.gov**\n* It has enough information to be able to construct a fips code column that will correspond the the NYT data\n* we will download the census data and stored it in /dbfs/tmp folder"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0b824ae-37a1-4f32-9178-66de3c2a7922"}}},{"cell_type":"code","source":["import pandas as pd\n\ncensus=pd.read_csv(\"https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/counties/totals/co-est2019-alldata.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56626243-aaad-4bb5-8a20-836bfffdf3ee"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Below command will not work in the community edition"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"faf2c6ff-53f6-4dcf-b816-bde8c0ff3fda"}}},{"cell_type":"code","source":["%sh wget https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/counties/totals/co-est2019-alldata.csv && cp co-est2019-alldata.csv dbfs/tmp"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"897e5dce-901d-407a-a9a4-2b6ad6cccb64"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### download the census data from [here](https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/counties/totals/co-est2019-alldata.csv) and import it into Databricks\n\nclick Data icon on the left and import file co-est2019-alldata.csv into default directory /FileStore/tables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a47d122-fb73-4f36-b3d0-ad4317c41e99"}}},{"cell_type":"code","source":["%fs ls /FileStore/tables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc3556ce-bdd8-4a5d-a1a0-a6d4edc8bbe2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["census_df = spark.read.csv(\"/FileStore/tables/co_est2019_alldata.csv\", header=True, inferSchema=True)\n\n#display() is a Databricks only function. It displays the data, like show(), but also gives the visualization options we saw in the SQL section above\ndisplay(census_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ec28cd4-1d00-42d1-be24-9b90f47c5151"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's tweak the DataFrame above to have a fips column that matches the NYT data. Here's the documentation on [user-defined functions (UDFs)](https://docs.databricks.com/spark/latest/spark-sql/udf-python.html)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38deddde-d272-4381-82e7-1cbf1ca518d9"}}},{"cell_type":"markdown","source":["### Create a UDF to generate fips based on state and county code in census data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f32c86b0-3a6b-4702-93e4-3a91d0df4267"}}},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\ndef make_fips(state_code, county_code):\n  if len(str(county_code)) == 1:\n    return str(state_code) + \"00\" + str(county_code)\n  elif len(str(county_code)) == 2:\n    return str(state_code) + \"0\" + str(county_code)\n  else:\n    return str(state_code) + str(county_code)\n\nmake_fips_udf = udf(make_fips, StringType())\n  \ncensus_df = census_df.withColumn(\"fips\", make_fips_udf(census_df.STATE, census_df.COUNTY))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f92d5900-a21d-42f5-9504-64d77be0392a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now that both the census and the covid data have an identical column, let's join the two DataFrames."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8fc1e81b-2318-46cf-bbda-1a2e9f06e6ab"}}},{"cell_type":"code","source":["covid_with_census = (covid_df\n                     .na.drop(subset=[\"fips\"])\n                     .join(census_df.drop(\"COUNTY\", \"STATE\"), on=['fips'], how='inner'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1d4bab3-75d9-48fe-8872-ac1234329bed"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["What do the cases look like for the most populous counties?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e105eeda-f7a5-42a2-a39a-841e9fccbc7d"}}},{"cell_type":"code","source":["display(covid_with_census.filter(\"POPESTIMATE2019 > 2000000\").select(\"county\", \"cases\", \"date\"))\n\n# keys = date, grouping = county, values = cases"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b191eb1-f128-466b-9057-eec08370b881"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Since the NYT dataset has a new row for every day, with cases increasing each day, let's grab only the most recent numbers for each county.\n* Below we're using the `col` function to refer to columns. It's equivalent to something like `df[\"column_name\"]`\n* To get the most recent row per county,  we'll use a [window function](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=window#pyspark.sql.Window)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8520ded0-421f-4628-a682-626ac1816c43"}}},{"cell_type":"code","source":["from pyspark.sql.functions import row_number, col\nfrom pyspark.sql import Window\n\nw = Window.partitionBy(\"fips\").orderBy(col(\"date\").desc())\ncurrent_covid_rates = (covid_with_census\n                       .withColumn(\"row_num\", row_number().over(w))\n                       .filter(col(\"row_num\") == 1)\n                       .drop(\"row_num\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ec17683-5e67-4350-b47b-5050f9134051"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["What counties are hardest hit when the cases are scaled with their population?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"068b93b9-1b07-416e-a2c1-f3e27939b1b2"}}},{"cell_type":"code","source":["current_covid_rates = (current_covid_rates\n                       .withColumn(\"case_rates_percent\", 100*(col(\"cases\")/col(\"POPESTIMATE2019\")))\n                       .sort(col(\"case_rates_percent\").desc()))\n\n#Look at the top 10 counties\ndisplay(current_covid_rates.select(\"county\", \"state\", \"cases\", \"POPESTIMATE2019\", \"case_rates_percent\").limit(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e85caaa-c2be-4a84-a42a-19bcb8be8564"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e944b602-fc78-4012-b0e2-64a3f41b9fb2"}},"outputs":[],"execution_count":0}],"metadata":{"name":"Intro to Spark","notebookId":6511114,"kernelspec":{"name":"python37364bitf1886774f0a5499190068f1e2da38f78","display_name":"Python 3.7.3 64-bit"},"application/vnd.databricks.v1+notebook":{"notebookName":"Week 02.3-Intro to Spark","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2647689195255449}},"nbformat":4,"nbformat_minor":0}
